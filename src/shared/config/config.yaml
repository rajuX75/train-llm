model:
  vocab_size: 8000
  hidden_size: 128
  intermediate_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  num_key_value_heads: 2
  max_position_embeddings: 512
  rope_theta: 10000.0
  layer_norm_eps: 1.0e-6
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  initializer_range: 0.02
  use_flash_attn: true
  use_rotary: true
  tie_word_embeddings: true
  gradient_checkpointing: true

training:
  data_dir: "./data"
  output_dir: "./models"
  cache_dir: "./cache"
  batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  min_learning_rate: 2.0e-5
  weight_decay: 0.01
  num_epochs: 100
  warmup_steps: 1000
  max_grad_norm: 1.0
  use_mixed_precision: true
  use_gradient_checkpointing: true
  use_compile: true
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  resume_from_checkpoint: null
  device: "auto"
  num_workers: 2
  seed: 42